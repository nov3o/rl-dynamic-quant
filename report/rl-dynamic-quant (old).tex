\documentclass{article}

% Required packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{balance}
\usepackage{hyperref}

% ICML style
\usepackage[accepted]{icml2018}

% For algorithms if needed
\usepackage{algorithm}
\usepackage{algorithmic}

% The short title for running headers
\icmltitlerunning{RL-Based Dynamic Quantization for LLMs}

\begin{document}

\twocolumn[
\icmltitle{RL-Based Dynamic Quantization for Large Language Models}

\begin{icmlauthorlist}
\icmlauthor{Oleg Roshka}{stanford}
\icmlauthor{Ilia Badanin}{epfl}
\end{icmlauthorlist}

\icmlaffiliation{stanford}{Department of Computer Science, Stanford University, Stanford, CA, USA}
\icmlaffiliation{epfl}{École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland}

\icmlcorrespondingauthor{Oleg Roshka}{oros@stanford.edu}
\icmlcorrespondingauthor{Ilia Badanin}{ilia.badadnin@epfl.ch}

\icmlkeywords{Reinforcement Learning, Quantization, Large Language Models, Memory Efficiency}

\vskip 0.3in
]

% Print affiliations footnote
\printAffiliationsAndNotice{}

\begin{abstract}
This milestone report presents progress on a dynamic quantization framework for large language models (LLMs). We employ reinforcement learning (RL) to adaptively choose per-layer quantization schemes (e.g., 4-bit or 8-bit) during fine-tuning. Experiments suggest the learned policy can preserve model performance while significantly reducing memory usage. We detail our reward design, preliminary results, and outline the next steps.
\end{abstract}

\section{Introduction}
Large language models (LLMs) can be prohibitively large for deployment in memory-constrained settings. Quantization---lowering numerical precision---offers a direct way to reduce model size and speed up inference. However, most solutions apply a \emph{uniform} bit-width (e.g.\ int8) across all layers \cite{Dettmers2022llm,Sun2023qlora}, which can lead either to wasted capacity if a layer could be safely quantized more aggressively, or to degraded performance if it is overly compressed.

To address these issues, we propose a \emph{Reinforcement Learning (RL)} approach that selects each layer's quantization type (e.g., \texttt{nf4}, \texttt{fp4}, \texttt{int8}, or \texttt{fp16}) \emph{dynamically} while fine-tuning on a small dataset. Our environment rewards the agent for maintaining model accuracy, preserving attention entropy, and maximizing memory savings. This method extends ideas from prior RL-based neural architecture or policy searches \cite{Zoph2017RL,Wang2020apq} and complements extreme LLM compression efforts \cite{egiazarian2024extreme,malinovskii2024pvtuning}.

\section{Related Work}

\section{Methodology}

\subsection{Reference Model Fine-Tuning}
First, we fine-tune GPT-2 on a reasoning dataset (e.g.\ CommonsenseQA). This fully fine-tuned GPT-2 (in FP32 or FP16) acts as our \emph{reference model}, providing $L_{\mathrm{ref}}$ (the reference loss) and $p_{\mathrm{ref}}$ (the reference distribution). This is similar to SFT (supervised fine-tuning) used for teacher logits in knowledge-distillation or reward-model contexts \cite{Sun2023qlora}.

\subsection{Reinforcement Learning Environment}
We clone the reference model weights into a \emph{quantized model}, which is adjusted layer-by-layer via RL. Each \emph{episode} involves:
\begin{enumerate}
	\item Iterating through all $N$ layers of the model in sequence.
	\item For each layer, the RL agent chooses a quantization scheme (one of \{\texttt{nf4}, \texttt{fp4}, \texttt{int8}, \texttt{fp16}\}).
	\item We apply that choice, then do a small amount of fine-tuning on a training minibatch.
	\item We measure partial validation loss and other metrics (KL, attention entropy) on the updated model.
\end{enumerate}
At the end of the episode, the environment computes a cumulative \emph{reward} capturing how well the chosen quantization policy balances performance and memory savings.

\subsection{Reward Function}
\label{sec:reward}
We define our reward $R$ as a sum of several terms, each corresponding to a different aspect of quality and efficiency in our quantized model. Specifically, we use:
\begin{align}
	R \;=\;& w_{\mathrm{perf}}\,\bigl(L_{\mathrm{ref}} - L_{\mathrm{quant}}\bigr)\; 
	\nonumber \\
	&\quad -\;w_{\mathrm{KL}}\,\mathrm{KL}\bigl(p_{\mathrm{quant}}\|\ p_{\mathrm{ref}}\bigr)
	\nonumber \\
	&\quad +\, w_{\mathrm{entropy}}\,\bigl(E_{\mathrm{quant}} - E_{\mathrm{ref}}\bigr)
	\nonumber \\
	&\quad \;+\; w_{\mathrm{memory}}\,\mathrm{MemSave}.
	\label{eq:reward}
\end{align}

\paragraph{Interpretation.}
\begin{itemize}
	\item $L_{\mathrm{ref}}$ and $L_{\mathrm{quant}}$ are the reference vs.\ quantized validation losses. 
	\item $\mathrm{KL}(p_{\mathrm{quant}}\|\!p_{\mathrm{ref}})$ encourages distribution alignment to the teacher (reference).
	\item $E_{\mathrm{quant}}, E_{\mathrm{ref}}$ measure attention entropies, preserving information flow \cite{Choi2018pact}.
	\item $\mathrm{MemSave}$ tracks fraction of bits saved, inspired by prior quantization frameworks \cite{Jacob2018quant}.
\end{itemize}
By combining these signals, the agent is simultaneously encouraged to match or improve upon the reference model's predictive capabilities, avoid diverging too far in probability space, retain higher attention diversity, and reduce model size.

\section{Implementation Details}
\textbf{Model \& Baseline.}
We currently focus on GPT-2 as a testbed. The reference model is the fully fine-tuned GPT-2 (e.g., FP16). A uniform 8-bit quantization serves as one baseline.

\textbf{RL Algorithm (PPO).}
We employ Proximal Policy Optimization \cite{ppo2017} for stability. The RL \emph{state} includes:
\begin{itemize}
	\item The current layer index
	\item Memory usage so far
	\item Historical bit choices for previous layers
	\item Running losses/KL vs.\ reference 
\end{itemize}
The policy outputs a discrete action (which quantization format to apply).

\section{Experiments and Results}

\subsection{Experimental Setup}

\subsection{Results}

\paragraph{Reward Over Episodes.}
Figure~\ref{fig:reward} shows total reward over 500 episodes. We see an upward trend from about 4--5 to over 9, suggesting the agent learns progressively better quantization decisions.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{example-image-a}
	\caption{Total RL reward vs.\ episode. The upward trajectory indicates improved policies balancing performance, KL, attention entropy, and memory savings.}
	\label{fig:reward}
\end{figure}

\paragraph{Loss Metrics.}
Figure~\ref{fig:losses} shows the final validation loss and the policy loss. The quantized model's validation loss remains close to that of the reference model, while the RL policy loss stabilizes near zero, indicating convergence.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{example-image-a}
	\caption{Validation loss (blue) vs.\ reference loss (green), and the PPO policy loss (orange). The quantized model stays near reference-level performance.}
	\label{fig:losses}
\end{figure}

\paragraph{Quantization Distribution.}
Finally, Figure~\ref{fig:quant-dist} shows how often each format is selected across episodes/layers. We see \texttt{fp4} and \texttt{nf4} are chosen most frequently, with smaller usage of \texttt{int8} and \texttt{fp16}. This implies the agent often prefers ultra-low precision for memory gains, without an excessive performance penalty.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95\columnwidth]{example-image-a}
	\caption{Histogram of chosen quantization types across all layers/episodes. 4-bit (\texttt{fp4}, \texttt{nf4}) is dominant.}
	\label{fig:quant-dist}
\end{figure}

\section{Discussion}

\subsection{Limitations}

\subsection{Future Work}
\paragraph{Hyperparameter Tuning.}
We will tweak $(w_{\mathrm{perf}}, w_{\mathrm{KL}}, w_{\mathrm{entropy}}, w_{\mathrm{memory}})$ to find stable regimes. Too large $w_{\mathrm{memory}}$ may push the policy to choose 4-bit everywhere, risking a performance drop.

\paragraph{Scaling Up.}
We plan to extend from GPT-2 to bigger open-source models, such as \texttt{deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B} or \texttt{microsoft/phi-2}, to test whether the method scales effectively on more challenging datasets. We also aim to run more comprehensive evaluations to compare standard quantized baselines against our dynamic approach (time and compute permitting).

\paragraph{Further Evaluation.}
We aim to measure:
\begin{itemize}
	\item \textbf{QA Accuracy} on more challenging datasets/benchmarks 
	\item \textbf{Memory usage} and \textbf{Inference latency}
	\item \textbf{Attention entropy} across deeper layers
\end{itemize}

\section{Conclusion}
We have shown an RL-driven approach to dynamic per-layer quantization that preserves near-reference performance while using predominantly 4-bit formats. Our multi-component reward (Equation~\ref{eq:reward}) guides the agent to choose lower precision whenever feasible, balancing KL, entropy, and memory. Ongoing tasks include hyperparameter tuning, scaling to larger GPT models, and extended evaluations on QA tasks.

\section*{Appendix}

% Use the ICML bibliography style
\bibliographystyle{icml2018}
\bibliography{bibliography}

\end{document}
