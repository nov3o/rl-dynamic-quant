\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{balance}
\usepackage{hyperref}
\usepackage[accepted]{icml2018}

% For algorithms/pseudocode
\usepackage{algorithm}
\usepackage{algorithmic}

% For tikz environment figure
\usepackage{tikz}
\usetikzlibrary{calc,arrows.meta,positioning,decorations.pathreplacing}

% The short title for running headers
\icmltitlerunning{DynaQuant: RL-Based Dynamic Quantization for LLMs}

\begin{document}
	
	\twocolumn[
	\icmltitle{DynaQuant: RL-Based Dynamic Quantization for Large Language Models}
	
	\begin{icmlauthorlist}
		\icmlauthor{Oleg Roshka}{stanford}
		\icmlauthor{Ilia Badanin}{epfl}
	\end{icmlauthorlist}
	
	\icmlaffiliation{stanford}{Department of Computer Science, Stanford University, Stanford, CA, USA}
	\icmlaffiliation{epfl}{École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland}
	
	\icmlcorrespondingauthor{Oleg Roshka}{oros@stanford.edu}
	\icmlcorrespondingauthor{Ilia Badanin}{ilia.badadnin@epfl.ch}
	
	\icmlkeywords{Reinforcement Learning, Quantization, Large Language Models, Memory Efficiency}
	
	\vskip 0.3in
	]
	
	% Print affiliations footnote
	\printAffiliationsAndNotice{}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% ABSTRACT
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{abstract}
		Large Language Models (LLMs) often suffer from excessive memory footprints and slow inference, creating barriers to deploying them on commodity devices or serving high-traffic scenarios. Quantization---using lower numerical precision---is a straightforward way to compress model size, but applying a \emph{uniform} precision (e.g., 8-bit) is suboptimal: some layers tolerate lower precision better than others. We propose \textbf{DynaQuant}, a Reinforcement Learning (RL) framework that \emph{dynamically} selects per-layer quantization schemes (e.g.\ 4-bit \texttt{nf4}, \texttt{fp4}, \texttt{int8}, or \texttt{fp16}) during short, iterative fine-tuning. Concretely, an RL agent observes each layer's statistics, memory usage, and partial performance signals to choose how to quantize that layer. We employ a multi-term reward that balances perplexity, KL divergence (vs.\ a reference model), attention entropy changes, and memory savings. Experiments on GPT-2 over BoolQ and PIQA show that DynaQuant finds \emph{mixed-precision} assignments that outperform uniform 4-bit or 16-bit baselines in perplexity and accuracy, while still yielding significant memory savings.
	\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Introduction}
	\label{sec:intro}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Recent progress in Large Language Models (LLMs) has driven state-of-the-art results in a variety of NLP tasks. However, the rapid growth in parameter counts (hundreds of millions to billions of parameters) poses nontrivial memory and runtime challenges, especially for resource-constrained deployments or high-throughput applications. \emph{Quantization} is a practical approach to reduce model size by lowering numerical precision (e.g.\ from float16 to 8-bit), enabling smaller memory footprints and often faster inference \cite{dettmers2022llmint8,frantar2022gptq}.
	
	Yet, standard uniform quantization assigns the same bit-width to all layers, which can be suboptimal. Some layers—particularly in attention or feed-forward blocks—are more sensitive to precision loss, while others can be safely compressed to 4-bit or even 2-bit with minimal accuracy degradation \cite{dong2019hawq,dettmers2023qlora}. This motivates \emph{mixed-precision quantization}, where each layer’s precision is chosen adaptively.
	
	We present \textbf{DynaQuant}, a reinforcement learning (RL) framework that \emph{sequentially} determines each layer’s quantization scheme. Concretely:
	\begin{enumerate}
		\item We treat each layer as a step in an RL episode.
		\item The RL agent picks one action from $\{\texttt{nf4}, \texttt{fp4}, \texttt{int8}, \texttt{fp16}\}$ for that layer.
		\item After quantizing, we do a short fine-tuning step on that partially quantized model to mitigate accuracy loss.
		\item We compute a \emph{reward} that balances perplexity (vs.\ a reference model), KL divergence, attention entropy changes, and memory savings.
	\end{enumerate}
	By the end of one \emph{episode} (quantizing all layers in sequence), we have a fully quantized model with a \emph{mixed-precision} assignment across layers.
	
	Empirically, we show that \textbf{DynaQuant} discovers policies that \emph{improve} perplexity and often accuracy relative to uniform 4-bit or 16-bit baselines, with memory usage that sits between those extremes. Our code is built on GPT-2 as a testbed and extends easily to other LLMs.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Related Work}
	\label{sec:related}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\paragraph{LLM Quantization.}
	Numerous works compress large models with low-precision formats: int8 \cite{dettmers2022llmint8}, 4-bit normal float (nf4) \cite{frantar2022gptq}, and others \cite{malinovskii2024pvtuning}. Typically, a \emph{uniform} scheme is used. Our approach is complementary, as we pick distinct bit-widths across layers via RL.
	
	\paragraph{Mixed-Precision and NAS.}
	Prior research in Neural Architecture Search (NAS) explores per-layer bit allocations \cite{dong2019hawq,Wang2020apq} but often focuses on smaller networks. We adapt these ideas for large Transformer-based LLMs, adding layer-by-layer \emph{short fine-tuning} and multi-objective RL signals (e.g.\ perplexity, KL, memory, attention).
	
	\paragraph{RL for Model Optimization.}
	RL has been employed to search network architectures \cite{zoph2016neural} or prune channels \cite{he2018amc}, but is less explored for dynamic LLM quantization. Our environment extends prior RL-based compression setups to handle short iterative fine-tuning, and an episodic reference model that closely mirrors the quantized model’s training steps.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Methodology}
	\label{sec:method}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{RL Environment: Dynamic Layer Quantization}
	We assume a reference model $\mathcal{M}_{\text{ref}}$ (e.g.\ GPT-2 in FP16) and a copy $\mathcal{M}_{\text{quant}}$ that we will quantize layer-by-layer. Each \textbf{episode} processes all $N$ layers in sequence, with each layer representing one RL step:
	\begin{enumerate}
		\item \textit{State} $s_i$: includes layer statistics (weight mean/std, gradient norms, attention entropy), the current layer index $i$, the model’s current perplexity, memory usage so far, and the previous layer’s quantization choice.
		\item \textit{Action} $a_i$: one of $\{\texttt{nf4}, \texttt{fp4}, \texttt{int8}, \texttt{fp16}\}$.
		\item \textit{Transition}: We quantize layer $i$ in $\mathcal{M}_{\text{quant}}$, then briefly fine-tune both $\mathcal{M}_{\text{ref}}$ and $\mathcal{M}_{\text{quant}}$ on the same minibatch.
		\item \textit{Reward} $r_i$: measures performance vs.\ memory trade-offs (details below).
	\end{enumerate}
	
	\begin{figure}[t]
		\centering
		\begin{tikzpicture}[>=latex,scale=0.62, every node/.style={transform shape}]
			% Nodes
			\node[draw, rectangle, minimum width=3.2cm, minimum height=1.2cm] (quantmodel) {\textbf{Quantized Model}};
			\node[draw, rectangle, right=1.8cm of quantmodel, minimum width=2.8cm, minimum height=1.2cm] (agent) {\textbf{RL Agent}};
			\node[draw, rectangle, below=1.3cm of agent, minimum width=2.5cm, minimum height=1.0cm] (eval) {Evaluate Loss/PPL};
			
			\node[draw, rectangle, fill=gray!20, minimum width=3.0cm, minimum height=1.2cm, left=1.8cm of quantmodel] (ref) {\textbf{Episodic Ref. Model}};
			\node[draw, rectangle, below=1.3cm of ref, minimum width=3.0cm, minimum height=1.2cm] (quant) {Apply Quant Type};
			
			% Arrows
			\draw[->, thick] (quantmodel) -- node[above]{\small state} (agent);
			\draw[->, thick] (agent) -- node[left]{\small act (qtype)} (quant);
			\draw[->, thick] (quant) -- node[left]{\small modify} (quantmodel);
			\draw[->, thick] (ref) -- node[right]{\small ref ppl} (eval);
			\draw[->, thick] (quantmodel) -- node[right]{\small quant ppl} (eval);
			\draw[->, thick] (eval) -- node[right]{\small reward} (agent);
			
			% Optional brace grouping: Fine-tuning
			\draw[decorate, decoration={brace, amplitude=10pt}]
			($(ref.north west)+(0.0,0.1)$) -- ($(quantmodel.north east)+(0.0,0.1)$)
			node[midway, above=16pt]{\small Fine-tune Both Models per Layer};
		\end{tikzpicture}
		\vspace{-0.5em}
		\caption{\small RL-based dynamic quantization loop. The agent picks a quant type for the current layer, we fine-tune the quantized and reference models, then compute performance signals and memory usage to form a reward.}
		\label{fig:rl-env}
		\vspace{-0.5em}
	\end{figure}
	
	\subsection{Reward Design}
	\label{subsec:reward}
	From the \textbf{final} version of our code, the \emph{reward} $r_i$ for layer $i$ is a weighted sum of four terms:
	
	\paragraph{(1) Performance (Perplexity) Reward.}
	We measure the perplexity difference between reference ($\mathrm{PPL}_\text{ref}$) and quantized ($\mathrm{PPL}_\text{quant}$) models:
	\begin{equation}
		r_{\mathrm{perf}} = (\mathrm{PPL}_\text{ref} \;-\; \mathrm{PPL}_\text{quant}) \;\times\; w_{\mathrm{perf}}.
		\label{eq:perf_term}
	\end{equation}
	If the quantized model has lower perplexity (compared to reference), $r_{\mathrm{perf}}$ is positive. Often, $\mathrm{PPL}_\text{quant}$ is higher, giving a negative contribution.
	
	\paragraph{(2) KL Divergence Penalty.}
	We compute $\mathrm{KL}(p_\text{quant}\,\|\,p_\text{ref})$ on a validation batch. A large KL means the quantized distribution diverges from reference:
	\begin{equation}
		r_{\mathrm{KL}} \;=\; -\, w_{\mathrm{KL}} \;\times\; \mathrm{KL}(p_\text{quant}\,\|\,p_\text{ref}).
	\end{equation}
	
	\paragraph{(3) Attention Entropy Preservation.}
	We compare the attention entropies in layer $i$: $E_\text{quant}$ vs.\ $E_\text{ref}$. Retaining diverse attention patterns is often crucial:
	\begin{equation}
		r_{\mathrm{entropy}} = (E_\text{quant} \;-\; E_\text{ref}) \times w_{\mathrm{entropy}}.
	\end{equation}
	
	\paragraph{(4) Memory Savings.}
	We measure how many bits are saved in layer $i$ relative to an FP16 baseline. If $a_i$ is $\texttt{nf4}$, we save $16-4=12$ bits per parameter, scaled by the fraction of total params in layer $i$:
	\begin{equation}
		r_{\mathrm{mem}} = \Bigl(\frac{16 - \text{bits}(a_i)}{16}\Bigr)\;\times \;\bigl(\text{layer\_size\_ratio}_i\bigr)\;\times w_{\mathrm{memory}}.
	\end{equation}
	
	Hence:
	\begin{align}
		r_i &= r_{\mathrm{perf}} + r_{\mathrm{KL}} + r_{\mathrm{entropy}} + r_{\mathrm{mem}}.
		\label{eq:reward_final}
	\end{align}
	
	\subsection{Policy Learning via PPO}
	We employ \textbf{Proximal Policy Optimization (PPO)} \cite{ppo2017} to update a small MLP policy $\pi_\theta$ that maps states to discrete actions (quantization types). At each new \emph{episode}, we:
	\begin{enumerate}
		\item \emph{Reset} the environment: copy the reference model to reinitialize $\mathcal{M}_\text{quant}$, set layer index to 0.
		\item For each layer $i=0 \dots N-1$:
		\begin{itemize}
			\item Agent picks $a_i \sim \pi_\theta(\cdot|s_i)$.
			\item We apply quantization type $a_i$ to layer $i$, do short fine-tuning, measure $(\mathrm{PPL}_\text{quant}, \mathrm{PPL}_\text{ref}, \mathrm{KL}, E_\text{quant}, E_\text{ref})$, compute reward $r_i$.
			\item Next state $s_{i+1}$ updated with new stats, layer index, etc.
		\end{itemize}
		\item We collect $(s_i,a_i,r_i)$ for all $i$, compute advantages (e.g.\ GAE), and run a few epochs of PPO updates on $\pi_\theta$.
	\end{enumerate}
	
	\vspace{-0.75em}
	\subsection{Algorithmic Pseudocode}
	\label{subsec:pseudocode}
	\vspace{-0.25em}
	
	\begin{algorithm}[h]
		\footnotesize
		\caption{DynaQuant (One PPO Iteration)}
		\label{alg:dynaquant}
		\begin{algorithmic}[1]
			\REQUIRE Model $\mathcal{M}_\text{ref}$ (baseline), RL policy $\pi_\theta$, reward weights $(w_{\mathrm{perf}}, w_{\mathrm{KL}}, w_{\mathrm{entropy}}, w_{\mathrm{memory}})$
			\STATE $\mathcal{M}_\text{quant} \leftarrow \text{clone of } \mathcal{M}_\text{ref}$
			\STATE $s_0 \leftarrow \textsc{InitState}()$; $i \leftarrow 0$; \text{done} $\leftarrow$ \textbf{False}
			\STATE \texttt{rollout} $\leftarrow []$
			\WHILE{not \text{done}}
			\STATE $a_i \sim \pi_\theta(a_i|s_i)$
			\STATE \textsc{QuantizeLayer}$(\mathcal{M}_\text{quant}, i, a_i)$
			\STATE \textsc{FineTune}($\mathcal{M}_\text{ref}, \text{dataBatch}, \text{epochs})$
			\STATE \textsc{FineTune}($\mathcal{M}_\text{quant}, \text{dataBatch}, \text{epochs})$
			\STATE $(r_i, \text{info}_i) \leftarrow \textsc{ComputeReward}(\mathcal{M}_\text{ref}, \mathcal{M}_\text{quant}, i)$
			\STATE $s_{i+1} \leftarrow \textsc{NextState}(\mathcal{M}_\text{quant}, i+1)$
			\STATE \texttt{rollout} $\leftarrow \texttt{rollout}\cup \{(s_i,a_i,r_i)\}$
			\STATE $i \leftarrow i+1$
			\IF{$i \ge N$}
			\STATE done $\leftarrow$ \textbf{True}
			\ENDIF
			\ENDWHILE
			\STATE \textsc{ComputeAdvantages}(\texttt{rollout})
			\STATE \textsc{UpdatePolicyPPO}($\pi_\theta$, \texttt{rollout})
		\end{algorithmic}
	\end{algorithm}
	
	Algorithm~\ref{alg:dynaquant} shows a single training iteration (episode). We typically repeat many episodes, reinitializing $\mathcal{M}_\text{quant}$ and $\mathcal{M}_\text{ref}$ each time.
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Experiments}
	\label{sec:experiments}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Setup and Datasets}
	\paragraph{Model.}
	We use GPT-2 (12-layer) as a proof-of-concept. The \emph{reference model} is GPT-2 in FP16, fully fine-tuned on CommonsenseQA. For RL episodes, we then quantize layer-by-layer on the same or similar data.
	
	\paragraph{Tasks.}
	We evaluate on \textbf{BoolQ} (yes/no QA) and \textbf{PIQA} (multiple-choice physical reasoning). We measure:
	\begin{itemize}
		\item \emph{Validation perplexity}
		\item \emph{Multiple-choice accuracy}
		\item \emph{Peak GPU memory usage} (MB)
		\item \emph{Inference throughput} (tokens/s)
	\end{itemize}
	
	\paragraph{Baselines.}
	We compare \textbf{Uniform FP16} and \textbf{Uniform NF4} with learned RL-based \emph{mixed} assignments discovered after PPO training.
	
	\subsection{Quantization Distributions and Reward Curves}
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\columnwidth]{example-image-a}
		\vspace{-1em}
		\caption{\small Example of total reward vs.\ episode. The agent steadily improves as it refines its per-layer quant decisions.}
		\label{fig:reward_curves}
		\vspace{-0.2em}
	\end{figure}
	
	\paragraph{Layer-Wise Heatmap.}
	Over many episodes, the RL policy converges to mostly 4-bit or 8-bit on many layers, with a few layers assigned to \texttt{fp16}. Figure~\ref{fig:heatmap} (schematic) shows how different layers end up being assigned specific bit-widths more frequently.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.93\columnwidth]{example-image-b}
		\vspace{-0.4em}
		\caption{\small Schematic heatmap showing quant type usage by layer across episodes. The RL policy typically picks lower bits for many layers, using higher precision on sensitive blocks.}
		\label{fig:heatmap}
		\vspace{-0.5em}
	\end{figure}
	
	
	\subsection{Results on BoolQ}
	
	\begin{table}[ht]
		\centering
		\caption{\small \textbf{BoolQ} (Validation). The first two rows are uniform baselines. The remaining rows are learned DynaQuant \emph{mixed} schemes.}
		\label{tab:boolq}
		\begin{tabular}{lcccc}
			\toprule
			\textbf{Method} & \textbf{PPL} & \textbf{Acc(\%)} & \textbf{Mem(MB)} & \textbf{Speed}\\
			\midrule
			\textbf{FP16 (baseline)}  & 920.56 & 51.93 & 422.69 & 48.8k\\
			\textbf{NF4 (uniform)}    & 873.65 & 52.02 & 300.95 & 42.8k\\
			\midrule
			\textbf{DynaQ Mix A}  & 777.13 & 52.08 & 464.29 & 39.3k\\
			\textbf{DynaQ Mix B}  & 773.78 & \textbf{53.85} & 383.15 & 40.4k\\
			\bottomrule
		\end{tabular}
		\vspace{-1em}
	\end{table}
	
	From Table~\ref{tab:boolq}, uniform NF4 beats FP16 in perplexity with notably lower memory usage. Our RL approach (\emph{Mix B}) lowers perplexity further \emph{and} boosts accuracy significantly, though at a slight memory overhead (383 MB vs.\ NF4’s 300 MB). Inference throughput also decreases somewhat, presumably due to mixing kernels.
	
	\subsection{Results on PIQA}
	
	\begin{table}[ht]
		\centering
		\caption{\small \textbf{PIQA} (Validation). The first two rows are uniform baselines. The remaining are RL-based mixed.}
		\label{tab:piqa}
		\begin{tabular}{lcccc}
			\toprule
			\textbf{Method} & \textbf{PPL} & \textbf{Acc(\%)} & \textbf{Mem(MB)} & \textbf{Speed}\\
			\midrule
			\textbf{FP16}    & 2531.21 & 60.72 & 422.69 & 48.2k \\
			\textbf{NF4}     & 2265.88 & 60.77 & 300.95 & 42.0k \\
			\midrule
			\textbf{DynaQ Mix C} & 2143.76 & \textbf{61.53} & 464.29 & 39.4k \\
			\textbf{DynaQ Mix D} & 2099.65 & 61.26 & 383.15 & 39.5k \\
			\bottomrule
		\end{tabular}
		\vspace{-0.5em}
	\end{table}
	
	Table~\ref{tab:piqa} shows a similar trend. \emph{Mix C} yields the highest accuracy (61.53\%) but also the highest memory usage among the listed approaches. \emph{Mix D} maintains a perplexity of 2099.65 and slightly lower accuracy, at a memory cost well below FP16. 
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Discussion}
	\label{sec:discussion}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Our experiments confirm that \emph{per-layer dynamic quantization} can surpass uniform quantization in perplexity or accuracy:
	\begin{itemize}
		\item \textbf{Uniform NF4} is already a strong baseline, typically offering better perplexity than FP16 in these tasks, plus $\sim$30\% memory savings.
		\item \textbf{DynaQuant improves} further, mixing \texttt{fp16} or \texttt{int8} for certain layers while using 4-bit for others. This yields additional perplexity gains and sometimes a tangible accuracy boost.
		\item \textbf{Speed trade-off}: Mixed precision can reduce throughput by requiring different kernel calls for different layers. 
	\end{itemize}
	
	\subsection{Limitations}
	\begin{itemize}
		\item \textbf{Compute Overhead}: Each RL episode fine-tunes \emph{all} layers, so total training cost is non-trivial.
		\item \textbf{Scalability}: We tested GPT-2. Larger LLMs (e.g.\ 1.5B–13B) might require more careful scheduling or partial-layer grouping to remain efficient.
		\item \textbf{Reward Calibration}: Weighting memory vs.\ perplexity vs.\ KL is subjective; tuning these hyperparameters is essential.
	\end{itemize}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Conclusion}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	We have presented \textbf{DynaQuant}, an RL-based, layer-by-layer quantization approach that adaptively decides which bit-width format to apply per Transformer layer. Our multi-term reward function---using perplexity difference, KL penalty, attention entropy preservation, and memory savings---guides the policy to compress the majority of layers aggressively while preserving or even boosting accuracy. On BoolQ and PIQA, DynaQuant’s \emph{mixed-precision} solutions outperform uniform quantization in perplexity/accuracy trade-offs, with memory footprints in-between purely 4-bit or purely 16-bit options.  
	
	\paragraph{Future Directions.}
	Ongoing and future extensions include:
	\begin{itemize}
		\item \textbf{Scaling to bigger LLMs}, e.g.\ 1.5B–7B parameters, analyzing the trade-off between policy complexity and training overhead.
		\item \textbf{Reward Tuning} for different tasks (e.g.\ generative chat, summarization).
		\item \textbf{Hardware-level optimizations}: investigating throughput on specialized GPU kernels or accelerators for mixed-precision inference.
		\item \textbf{Integration with quantization-aware fine-tuning frameworks}: combining DynaQuant’s layer decisions with advanced data augmentation or knowledge distillation.
	\end{itemize}
	
	\balance
	\bibliographystyle{icml2018}
	\bibliography{bibliography}
	
\end{document}
